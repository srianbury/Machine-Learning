{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> Download\n",
      "Command 'Download' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"Tweets.csv\")\n",
    "data = data[['airline_sentiment', 'text']] # use a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_and_remove_stopwords(tweet):\n",
    "    tweet_words_sans_stopwords = []\n",
    "    tweet_no_at = ' '.join(filter(lambda word:word[0]!='@', tweet.split())) #filter out words that start with @\n",
    "    tweet_with_letters_only = re.sub(\"[^a-zA-Z]\", \" \", tweet_no_at) #substitute any character that isnt a-z or A-Z with \" \"\n",
    "    tweet_with_letters_only_and_all_lower_case = tweet_with_letters_only.lower() #make all words lowercase\n",
    "    list_of_words =  tweet_with_letters_only_and_all_lower_case.split() #turn the string into a list of words\n",
    "    for word in list_of_words:\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            tweet_words_sans_stopwords.append(word)\n",
    "    tweet_sans_stopwords = \" \".join(tweet_words_sans_stopwords)\n",
    "    return tweet_sans_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica What @dhepburn said.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_REVIEWS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cleaned_reviews = []\n",
    "for index in range(len(data[\"text\"][:NUMBER_OF_REVIEWS])):\n",
    "    list_of_cleaned_reviews.append(clean_text_and_remove_stopwords(data[\"text\"][index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_tweets, test_tweets, train_target, test_target) = \\\n",
    "train_test_split(list_of_cleaned_reviews, data[\"airline_sentiment\"][:NUMBER_OF_REVIEWS],\\\n",
    "test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'abbreve', 'abc', 'ability', 'able', 'absolute', 'absolutely', 'absurd', 'absurdly', 'abt', 'abundance', 'access', 'accident', 'accommodate', 'accompany', 'according', 'account', 'achieves', 'across', 'action', 'active', 'actual', 'actually', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'addresses', 'adds', 'addtl', 'adopting', 'adore', 'adult', 'advance', 'advantage', 'advise', 'advising', 'advisory', 'aeroport', 'aesthetics', 'affected', 'affiliates', 'afford', 'afiliates', 'afternoon', 'agent', 'agents', 'agfd', 'aggressive', 'agnt', 'ago', 'agr', 'ah', 'ahead', 'air', 'aircraft', 'airline', 'airlines', 'airplane', 'airplanemodewason', 'airport', 'airports', 'allan', 'almost', 'alone', 'along', 'already', 'also', 'always', 'alwaysdelayedonunited', 'amaze', 'amazing', 'amazingly', 'amazings', 'amenity', 'america', 'american', 'americanairlines', 'amp', 'andchexmix', 'andrews', 'angeles', 'angry', 'announce', 'announced', 'announcement', 'announces', 'annoyed', 'another', 'answer', 'answering', 'answers', 'anxiety', 'anymore', 'anyone', 'anyonethere', 'anything', 'anytime', 'anyway', 'anywhere', 'aormfkac', 'apart', 'apathy', 'apologies', 'apology', 'app', 'apparently', 'appear', 'appears', 'apple', 'application', 'applied', 'appreciate', 'appropriate', 'apps', 'apr', 'apt', 'aptzpurop', 'aqjn', 'aqzwecokk', 'arab', 'area', 'argument', 'arms', 'around', 'arrival', 'arrive', 'arrived', 'arrives', 'arriving', 'artisanal', 'asap', 'ase', 'ask', 'asked', 'asking', 'assaulted', 'assets', 'assist', 'assistance', 'assistant', 'assume', 'assuming', 'assured', 'atfrkp', 'atl', 'atlanta', 'atlantic', 'atrocious', 'attached', 'attained', 'attendant', 'attendants', 'atx', 'au', 'aus', 'austin', 'australia', 'austrian', 'auto', 'avail', 'availab', 'available', 'avalonhollywood', 'avatars', 'avgeek', 'aviv', 'avoid', 'avoided', 'avyqdmpi', 'awaiting', 'award', 'awards', 'away', 'awdbw', 'awesome', 'awful', 'awhile', 'aztdaer', 'baby', 'back', 'backtowinter', 'backyard', 'bad', 'badcustomerservice', 'badges', 'bag', 'baggage', 'baggageissues', 'bags', 'balancing', 'baldwin', 'bank', 'banned', 'based', 'battle', 'battling', 'bc', 'bday', 'beans', 'beat', 'beats', 'beatstheothers', 'beautiful', 'beautifull', 'beautifully', 'become', 'begin', 'begrudgingly', 'behalf', 'behind', 'belfast', 'bench', 'benefits', 'bergstrom', 'best', 'bestcrew', 'bet', 'better', 'bettween', 'beyond', 'bff', 'bg', 'big', 'bill', 'bin', 'bins', 'bio', 'bird', 'birthday', 'bit', 'biz', 'biztravel', 'bked', 'black', 'blackmailed', 'blame', 'blaming', 'blast', 'blasting', 'blazer', 'blew', 'board', 'boarded', 'boarding', 'bonuses', 'book', 'booked', 'booking', 'boom', 'boost', 'booted', 'bos', 'boston', 'bot', 'bothered', 'bought', 'bounce', 'bound', 'boy', 'boyfriend', 'brain', 'brand', 'branson', 'break', 'breaks', 'briefings', 'brilliant', 'bring', 'brisk', 'broken', 'brokenwheel', 'browser', 'browsers', 'bruh', 'btw', 'bubbly', 'bucks', 'budapest', 'bug', 'built', 'bullshit', 'bumping', 'bunch', 'burg', 'burgundy', 'business', 'businesstravel', 'butt', 'button', 'buy', 'buzj', 'bwi', 'ca', 'cabaret', 'cabin', 'cabo', 'caching', 'caffeine', 'cake', 'california', 'call', 'called', 'calling', 'calls', 'calming', 'came', 'canada', 'canadian', 'cancelled', 'cancer', 'cancun', 'canned', 'cannot', 'cant', 'captain', 'car', 'card', 'care', 'cared', 'caring', 'carousel', 'carrie', 'carriers', 'carrieunderwood', 'carry', 'carrying', 'carryon', 'carseat', 'case', 'catch', 'category', 'caught', 'cause', 'caused', 'causes', 'ceases', 'celebrating', 'center', 'centers', 'central', 'century', 'certificate', 'cessna', 'cf', 'challenge', 'challenges', 'chance', 'change', 'changed', 'channel', 'channels', 'charge', 'charged', 'charges', 'charging', 'charlotte', 'chase', 'chat', 'cheapflights', 'check', 'checked', 'checkin', 'checking', 'checkout', 'cheers', 'cheese', 'chicago', 'chicken', 'child', 'china', 'chinese', 'chng', 'choice', 'choosekind', 'choppy', 'cih', 'cities', 'city', 'ckg', 'claim', 'claimed', 'claims', 'class', 'classics', 'classiq', 'classy', 'cle', 'clear', 'clicking', 'client', 'clients', 'closed', 'closest', 'clt', 'club', 'clue', 'cmh', 'cnctl', 'co', 'coach', 'coasts', 'coats', 'code', 'codeshare', 'coffee', 'cold', 'colo', 'com', 'come', 'comenity', 'comfort', 'comfortable', 'coming', 'comment', 'commercial', 'committed', 'common', 'communication', 'communicationfail', 'company', 'compatible', 'compensation', 'competes', 'competition', 'complaint', 'complete', 'completely', 'compliment', 'complimentary', 'comps', 'computer', 'computers', 'concern', 'concerned', 'concrete', 'conf', 'confident', 'confirm', 'confirmation', 'confirmed', 'congrats', 'connect', 'connecting', 'connection', 'connections', 'connectns', 'connolly', 'consider', 'considerable', 'considering', 'consistent', 'consolation', 'constant', 'constantly', 'construction', 'consumers', 'contact', 'contacted', 'contacting', 'continental', 'continentalairlines', 'continue', 'continued', 'controlled', 'convenient', 'conversion', 'cool', 'corp', 'correct', 'correctness', 'cost', 'costs', 'could', 'count', 'counter', 'country', 'couple', 'coupon', 'course', 'courtesy', 'cousin', 'cozy', 'cpap', 'cqmm', 'crap', 'creates', 'credit', 'crew', 'crisis', 'cross', 'crossed', 'cruz', 'cs', 'css', 'ct', 'cue', 'current', 'currently', 'cus', 'cust', 'customer', 'customers', 'customerservice', 'cut', 'cuz', 'cyberattack', 'dad', 'daily', 'dal', 'dallas', 'damage', 'damn', 'dan', 'dancing', 'date', 'daughter', 'david', 'day', 'days', 'daystogo', 'dc', 'dca', 'dcobokn', 'dead', 'deal', 'dealing', 'deals', 'dear', 'death', 'debacle', 'debbie', 'december', 'decisions', 'decorum', 'deep', 'defiantly', 'defines', 'definitely', 'degrees', 'deicing', 'delay', 'delayed', 'delaying', 'delays', 'delhi', 'delighted', 'delivered', 'delivery', 'delta', 'delyd', 'demand', 'demo', 'demoted', 'den', 'denied', 'denver', 'depart', 'departing', 'departure', 'dept', 'deserved', 'deserves', 'design', 'designated', 'desk', 'desktop', 'despite', 'destination', 'destinations', 'destroyed', 'details', 'develop', 'device', 'devices', 'dfw', 'dhn', 'dicks', 'didnt', 'diego', 'diehardvirgin', 'diff', 'different', 'difficult', 'digging', 'direct', 'directly', 'dirtiest', 'dirty', 'disability', 'disabled', 'disappeared', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'discrimination', 'discuss', 'disgutedindenver', 'dislike', 'dispatch', 'disproportionate', 'disputed', 'disrespect', 'disrupted', 'disruption', 'distress', 'distribution', 'diverted', 'dm', 'dmed', 'dms', 'dnstitrzwy', 'doctor', 'doesnt', 'dog', 'domestic', 'donate', 'done', 'donkey', 'dontdothistome', 'doom', 'doors', 'douglas', 'downtown', 'dqbai', 'dream', 'dreaming', 'dress', 'drfaoq', 'drink', 'drinker', 'drinks', 'drive', 'drivers', 'driving', 'drop', 'dropped', 'droppeditoffyet', 'dtw', 'dude', 'due', 'duffle', 'dulles', 'duw', 'dvt', 'dw', 'eagle', 'ear', 'earlier', 'early', 'earned', 'earphone', 'easily', 'eastern', 'easy', 'eat', 'ebk', 'economy', 'ee', 'ef', 'efficiently', 'efforts', 'either', 'elevate', 'elevategold', 'elevateuser', 'ella', 'else', 'elsewhere', 'email', 'emailed', 'emailing', 'emails', 'embossed', 'emergency', 'emirates', 'employee', 'employees', 'encountered', 'encouraged', 'end', 'ends', 'enforcement', 'eniqg', 'enjoyed', 'enough', 'enroll', 'ensure', 'enter', 'entered', 'entertaining', 'entertainment', 'entire', 'enuf', 'eps', 'equals', 'equipment', 'eri', 'error', 'errors', 'eservice', 'esp', 'etailwest', 'etc', 'ethiopia', 'etihad', 'europe', 'even', 'evening', 'ever', 'every', 'everyone', 'everything', 'ewr', 'ex', 'exact', 'exactly', 'example', 'exceed', 'excellent', 'excited', 'excuses', 'executives', 'exhausted', 'exist', 'existent', 'existing', 'exp', 'expanding', 'expansion', 'expect', 'expectations', 'expected', 'expense', 'expensive', 'experience', 'experienced', 'experiences', 'expired', 'expires', 'explain', 'exposed', 'express', 'extortion', 'extra', 'extremely', 'ey', 'eye', 'eyj', 'face', 'faces', 'facing', 'fact', 'fail', 'failing', 'failure', 'failures', 'fairs', 'falling', 'family', 'far', 'fare', 'farecompare', 'fares', 'fargo', 'faster', 'fat', 'fav', 'favorite', 'fb', 'fc', 'fcmostinnovative', 'fe', 'feat', 'feb', 'february', 'fee', 'feedback', 'feel', 'feels', 'fees', 'feet', 'female', 'fend', 'fgrbpazsix', 'fianc', 'field', 'fight', 'figure', 'file', 'filed', 'filled', 'films', 'final', 'finalizing', 'finally', 'financial', 'find', 'finding', 'fine', 'finest', 'fingers', 'first', 'fit', 'fits', 'fix', 'flat', 'fleet', 'flew', 'flex', 'flh', 'flies', 'flight', 'flightations', 'flighted', 'flighting', 'flightlation', 'flightled', 'flightn', 'flightr', 'flights', 'fll', 'floor', 'floors', 'flown', 'flt', 'flwmgdahxu', 'fly', 'flyer', 'flyers', 'flying', 'folk', 'folks', 'follow', 'followed', 'food', 'forcing', 'forecasts', 'forget', 'forgot', 'form', 'forth', 'forward', 'found', 'four', 'fq', 'fran', 'francisco', 'frankly', 'free', 'freeneversucks', 'french', 'frequency', 'frequent', 'freyasfund', 'friday', 'friendlyskies', 'friends', 'frigid', 'frm', 'front', 'frustrated', 'frustrating', 'ftw', 'fucked', 'fuel', 'full', 'fun', 'funds', 'funny', 'funnycaptain', 'furrow', 'future', 'fvudmh', 'fyi', 'ga', 'gaga', 'game', 'gate', 'gates', 'gave', 'gdg', 'gee', 'generic', 'gesture', 'get', 'getaway', 'getmartyhome', 'getting', 'gf', 'giants', 'gift', 'girl', 'give', 'given', 'gives', 'giving', 'gjt', 'gla', 'glad', 'glasgow', 'glitch', 'global', 'globe', 'glxfwp', 'gm', 'go', 'goes', 'going', 'gold', 'golds', 'gone', 'gong', 'gonmrwem', 'gonna', 'gonnabealongnight', 'good', 'goodenoughmother', 'goodness', 'goodnight', 'got', 'gotta', 'goy', 'grand', 'grandma', 'graphic', 'graphics', 'gratitude', 'great', 'greeting', 'greetings', 'greetingz', 'greyed', 'grezp', 'gross', 'ground', 'group', 'grouping', 'grp', 'grrwaaa', 'gsb', 'gt', 'gtdwpk', 'guess', 'guests', 'guiltypleasures', 'guitars', 'gusty', 'guys', 'guyyyys', 'gxdqortss', 'hahaha', 'half', 'hand', 'handed', 'handily', 'handle', 'hands', 'hannah', 'happen', 'happened', 'happening', 'happens', 'happily', 'happy', 'haqc', 'hard', 'hare', 'hates', 'hats', 'haul', 'hawaii', 'head', 'headed', 'header', 'headphones', 'hear', 'heard', 'heat', 'heaven', 'help', 'helped', 'helpful', 'helpimstuck', 'helping', 'helpmeplease', 'hence', 'herbal', 'hey', 'heyyyy', 'hi', 'highly', 'hipster', 'hiring', 'hisc', 'hmm', 'hold', 'holiday', 'holla', 'hollow', 'hollywood', 'home', 'honor', 'hop', 'hope', 'hoping', 'horrible', 'hot', 'hotel', 'hotl', 'hotline', 'hotspot', 'hou', 'hour', 'hours', 'houston', 'hpsqvrjk', 'hr', 'hrs', 'html', 'http', 'https', 'hubby', 'hugely', 'human', 'hundreds', 'hung', 'husband', 'hwnac', 'hy', 'iad', 'iah', 'ibtr', 'ice', 'iced', 'iconography', 'icu', 'idea', 'ignored', 'iljaebv', 'ill', 'illegal', 'ilw', 'im', 'imagine', 'impact', 'impending', 'impossible', 'impress', 'impressive', 'improve', 'improvement', 'inadequate', 'inappropriate', 'inappropriately', 'incident', 'incls', 'include', 'included', 'inconvenience', 'inconvenient', 'inconveniently', 'increased', 'incredible', 'incredibly', 'incubator', 'incur', 'indicates', 'indication', 'industry', 'infant', 'inflight', 'info', 'infographic', 'information', 'informative', 'inline', 'innovation', 'inop', 'inquired', 'inquiries', 'instead', 'instructed', 'int', 'intact', 'integrate', 'interesting', 'intern', 'international', 'internet', 'intl', 'intro', 'investigated', 'investor', 'ios', 'iove', 'ipad', 'iphone', 'irmafromdallas', 'isis', 'issue', 'issues', 'item', 'items', 'itinerary', 'jac', 'jack', 'jacket', 'jan', 'january', 'javascript', 'jbmvvha', 'jeu', 'jfk', 'jh', 'jk', 'jkf', 'jloiblnair', 'job', 'joke', 'jose', 'jp', 'jperhi', 'jq', 'julie', 'july', 'jump', 'jv', 'kacy', 'karen', 'keep', 'keeper', 'keeps', 'kept', 'ker', 'kevin', 'kewl', 'kf', 'khx', 'kicked', 'kids', 'killed', 'kinda', 'kindle', 'kits', 'kitty', 'kj', 'km', 'kmm', 'kn', 'kncpf', 'knew', 'know', 'knows', 'kooples', 'kp', 'kqnrrp', 'kul', 'kwm', 'la', 'lack', 'ladan', 'lady', 'lame', 'land', 'landed', 'landing', 'lands', 'large', 'larkc', 'las', 'last', 'late', 'latrice', 'laughed', 'law', 'lax', 'layover', 'lbcncr', 'lbs', 'learn', 'learncustomerservice', 'least', 'leave', 'leaving', 'left', 'leg', 'less', 'let', 'lets', 'letsgohome', 'letting', 'levels', 'lfs', 'lfulcbq', 'lga', 'lhr', 'license', 'lie', 'life', 'light', 'lighting', 'like', 'likely', 'likingyoulessandless', 'line', 'lineup', 'link', 'list', 'listing', 'literally', 'literate', 'little', 'livewelltraveled', 'load', 'loaded', 'loads', 'loaner', 'local', 'location', 'lock', 'loft', 'logan', 'logic', 'lol', 'lone', 'loner', 'long', 'longer', 'look', 'looking', 'looks', 'los', 'lose', 'lost', 'lostluggage', 'lot', 'lots', 'lounge', 'love', 'loved', 'lovely', 'low', 'loyal', 'lt', 'lucas', 'luck', 'luckily', 'lufthansa', 'luggage', 'lusaka', 'luv', 'luxuries', 'lwwdac', 'mac', 'machine', 'made', 'mail', 'main', 'maintenance', 'major', 'make', 'makes', 'making', 'male', 'malfunction', 'mammoth', 'man', 'manage', 'manchester', 'maneuver', 'manner', 'manually', 'many', 'march', 'market', 'marketing', 'married', 'martin', 'mary', 'match', 'matter', 'may', 'maybe', 'mayweatherpacquiao', 'mcdonnell', 'mco', 'mde', 'mean', 'means', 'measly', 'mechanical', 'mechanics', 'media', 'medical', 'meet', 'meeting', 'meetings', 'meetthefleet', 'member', 'members', 'membership', 'men', 'mention', 'mentioned', 'mere', 'merger', 'message', 'messages', 'messed', 'met', 'method', 'mexico', 'mia', 'miami', 'middle', 'middleeast', 'midnight', 'might', 'mileage', 'mileageplus', 'miles', 'military', 'min', 'mind', 'minimal', 'minor', 'mins', 'minus', 'minute', 'minutes', 'mismanagement', 'miss', 'missed', 'missing', 'mistake', 'mistakes', 'mobile', 'mobility', 'model', 'modify', 'mom', 'moment', 'momma', 'mon', 'monday', 'money', 'month', 'months', 'mood', 'moodlight', 'moodlighting', 'moodlitmonday', 'morning', 'mostly', 'mountains', 'move', 'moved', 'movie', 'moving', 'mph', 'mpower', 'mr', 'msy', 'much', 'multiple', 'music', 'must', 'mwpg', 'mysterious', 'myvxexperience', 'nada', 'naelah', 'nah', 'name', 'narayanan', 'nd', 'ndmgz', 'nearly', 'necessity', 'need', 'needed', 'needing', 'needs', 'nef', 'neglect', 'negligent', 'negotiate', 'nerd', 'nerdbird', 'nervous', 'network', 'neurosurgery', 'never', 'neveragain', 'neverflyvirgin', 'neverflyvirginforbusiness', 'new', 'newark', 'newly', 'news', 'next', 'nf', 'nice', 'nicely', 'night', 'nightmare', 'noair', 'nobody', 'nominated', 'non', 'noneother', 'nonrefundable', 'nonsense', 'nonstop', 'noob', 'noooo', 'nope', 'norm', 'normal', 'notfair', 'nothing', 'notice', 'notification', 'notifications', 'nq', 'nqh', 'nue', 'nullified', 'number', 'numbers', 'numofpointsavailable', 'nuts', 'nwk', 'ny', 'nyc', 'nyprzl', 'oa', 'oaflfr', 'oardjjgrrd', 'obnoxious', 'obsessed', 'obvious', 'october', 'od', 'offer', 'offered', 'office', 'ogg', 'oh', 'ohare', 'ok', 'okay', 'okc', 'old', 'omg', 'onboad', 'one', 'ones', 'online', 'open', 'opened', 'opens', 'operation', 'operational', 'opportunity', 'option', 'options', 'ord', 'order', 'ordered', 'orf', 'original', 'orlando', 'os', 'oscars', 'oscarscountdown', 'others', 'otherwise', 'ourprincess', 'outbound', 'outcome', 'outlets', 'outlook', 'outside', 'outstanding', 'overbook', 'overbooked', 'overhead', 'overnight', 'overwhelmed', 'owed', 'pa', 'pacific', 'pack', 'page', 'paid', 'painless', 'pair', 'pairing', 'pairings', 'palm', 'panicked', 'paper', 'paris', 'parody', 'part', 'partner', 'partners', 'pasengers', 'pass', 'passbook', 'passenger', 'passengers', 'password', 'past', 'patches', 'pattern', 'pax', 'pay', 'paying', 'payment', 'payments', 'pays', 'pbamu', 'pdx', 'peeps', 'pefe', 'penalty', 'people', 'perfect', 'performance', 'permanently', 'permission', 'persisting', 'person', 'personal', 'pesky', 'pf', 'ph', 'philadelphia', 'philly', 'phl', 'phn', 'phoenix', 'phone', 'phones', 'photo', 'pic', 'picture', 'pillows', 'pilot', 'pilots', 'pink', 'pioa', 'pj', 'placing', 'plague', 'plain', 'plane', 'planes', 'planned', 'planning', 'plans', 'plate', 'platinum', 'platter', 'playing', 'please', 'pleasecomeback', 'pleased', 'ploughs', 'pls', 'plus', 'pm', 'pnbajfkmhg', 'point', 'points', 'policy', 'politely', 'poor', 'pop', 'popular', 'portfolio', 'portland', 'pos', 'position', 'positive', 'possible', 'post', 'posted', 'potentially', 'power', 'ppl', 'pqd', 'pre', 'preboard', 'precipitation', 'preexisting', 'prefer', 'preferred', 'premier', 'premium', 'prepares', 'pressure', 'pretty', 'prevent', 'prevented', 'previous', 'price', 'prime', 'prince', 'princesshalf', 'prior', 'priority', 'probably', 'problem', 'problems', 'process', 'product', 'profitability', 'program', 'projects', 'promised', 'promises', 'promo', 'promoting', 'prompt', 'proper', 'properly', 'pros', 'prove', 'provide', 'provided', 'ps', 'psp', 'pt', 'publicly', 'pujvcelng', 'pull', 'purchase', 'purchased', 'put', 'putting', 'pxdel', 'pxexilsjbs', 'pyalebgkjt', 'qdebyahqfm', 'qeg', 'qm', 'qmdfqgf', 'qnllcm', 'qsqmm', 'question', 'queue', 'quick', 'quickly', 'quiet', 'quite', 'quzvmk', 'qxnoaqtyn', 'race', 'raeann', 'rain', 'raise', 'raising', 'ran', 'random', 'randomly', 'rang', 'rarely', 'rate', 'rates', 'rather', 'ray', 'rd', 'reach', 'reaches', 'read', 'ready', 'real', 'realistically', 'really', 'rearrange', 'reason', 'rebook', 'rebooting', 'rec', 'receipt', 'receive', 'received', 'recent', 'recheck', 'recieved', 'recline', 'recommend', 'recommended', 'record', 'recouping', 'recourse', 'rectify', 'red', 'redesign', 'redirected', 'reduce', 'redwineisbetter', 'ref', 'referencing', 'reflight', 'refreshed', 'refreshing', 'refund', 'refuse', 'refusing', 'regarding', 'register', 'regulation', 'regulations', 'reimburse', 'reimbursement', 'reinstated', 'reissue', 'reluctant', 'remember', 'remembered', 'remorse', 'removal', 'reno', 'rep', 'repair', 'repeatably', 'repeated', 'replacement', 'reply', 'report', 'reports', 'representative', 'representatives', 'represents', 'reps', 'reputation', 'requested', 'requests', 'required', 'requirement', 'requires', 'reschedule', 'rescheduled', 'rescheduling', 'resent', 'reserv', 'reservation', 'reservations', 'reserved', 'reset', 'resolution', 'resolutions', 'resolve', 'resolved', 'respond', 'responded', 'responding', 'response', 'responses', 'responsibility', 'ressie', 'restr', 'resulted', 'results', 'resume', 'return', 'returned', 'returning', 'returns', 'revenue', 'reviewed', 'revisiting', 'rgywjbbhm', 'rhkamx', 'richard', 'ride', 'ridiculous', 'ridiculousness', 'right', 'rim', 'rise', 'risk', 'rlz', 'rmznivgmg', 'road', 'roasted', 'rock', 'rockies', 'rockstar', 'rockstars', 'rookie', 'room', 'rooms', 'rosetta', 'rough', 'round', 'route', 'routes', 'row', 'rozana', 'rpdbpx', 'rps', 'rt', 'rtb', 'rtr', 'rude', 'ruined', 'rules', 'run', 'runaround', 'running', 'sabe', 'sad', 'safari', 'safety', 'said', 'salisbury', 'san', 'sandiego', 'sanfrancisco', 'santa', 'sarcasm', 'sarcastically', 'sat', 'saturday', 'save', 'saving', 'saw', 'say', 'saying', 'says', 'scam', 'scared', 'sched', 'schedule', 'scheduled', 'sciencebehindtheexperience', 'scolding', 'screen', 'screening', 'screws', 'sea', 'search', 'season', 'seat', 'seated', 'seating', 'seats', 'seattle', 'sec', 'second', 'section', 'secure', 'security', 'see', 'seeing', 'seeking', 'seem', 'seems', 'seen', 'sees', 'segway', 'select', 'selected', 'selecting', 'selects', 'self', 'send', 'sendambien', 'sent', 'sentinel', 'september', 'serious', 'seriously', 'serv', 'service', 'servicedog', 'services', 'set', 'several', 'severe', 'sf', 'sfjduahx', 'sfo', 'sfotobos', 'shaking', 'shall', 'shame', 'share', 'shared', 'shares', 'sharing', 'shattered', 'sherocks', 'shift', 'shit', 'shocked', 'shocking', 'shoes', 'short', 'shot', 'show', 'showed', 'shower', 'shows', 'sick', 'sign', 'signed', 'silicon', 'silly', 'silver', 'simple', 'simply', 'since', 'sincerely', 'singer', 'sit', 'site', 'sitting', 'situation', 'sjc', 'sked', 'skiing', 'sleep', 'sllyibe', 'slots', 'small', 'smh', 'smooth', 'sna', 'sneaky', 'snow', 'soft', 'software', 'sold', 'solution', 'solving', 'somehow', 'someone', 'something', 'son', 'song', 'soon', 'sooner', 'soooo', 'soreback', 'sorry', 'sorrynotsorry', 'sort', 'sorted', 'sos', 'sounds', 'southwest', 'southwestair', 'spare', 'speak', 'special', 'spend', 'spending', 'spent', 'split', 'spoil', 'spoiled', 'spoke', 'spot', 'spotify', 'spousal', 'spread', 'springs', 'sprint', 'st', 'staff', 'standards', 'standby', 'standing', 'starbucks', 'start', 'started', 'starting', 'stat', 'statement', 'states', 'status', 'statusmatch', 'statusmatchpaidoff', 'stay', 'steel', 'steep', 'stellar', 'step', 'steve', 'stewardess', 'sti', 'still', 'stink', 'stl', 'stolen', 'stood', 'stop', 'stopped', 'storm', 'story', 'stranded', 'stressed', 'stressful', 'strong', 'stuck', 'stuff', 'stunned', 'stylesheets', 'submitted', 'subsequent', 'suck', 'sucked', 'sucks', 'sugafly', 'suitcase', 'sun', 'sunset', 'super', 'superior', 'supervisor', 'supp', 'support', 'supposed', 'sure', 'surgeries', 'surgery', 'surly', 'suspect', 'svc', 'svcs', 'swanky', 'sweet', 'switched', 'system', 'systemwide', 'sytycd', 'szr', 'ta', 'tables', 'tag', 'tags', 'tailwind', 'take', 'taken', 'takeoff', 'takes', 'taking', 'talk', 'talking', 'tarmac', 'taxes', 'tea', 'team', 'tech', 'tel', 'tell', 'telling', 'tells', 'temperature', 'terminal', 'terrible', 'terrific', 'texas', 'text', 'tfanxbh', 'th', 'thai', 'thaiairways', 'thank', 'thankfully', 'thanks', 'thankyou', 'thanx', 'thats', 'thestarter', 'theworst', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'thnx', 'tho', 'though', 'thought', 'threatens', 'three', 'thrombosis', 'thrown', 'thru', 'thurs', 'thursday', 'thx', 'ticket', 'tickets', 'tide', 'tides', 'time', 'timely', 'times', 'tinderchamp', 'tindertips', 'tired', 'tiredofthis', 'tix', 'tkvmhbkec', 'tlh', 'tmm', 'tmusjvu', 'today', 'todays', 'together', 'told', 'tomorrow', 'tonight', 'tonite', 'took', 'tool', 'top', 'topnews', 'tossed', 'total', 'totally', 'touch', 'touchdown', 'touched', 'tough', 'tourist', 'toward', 'towards', 'track', 'tracking', 'train', 'training', 'transactional', 'transatlantic', 'transfer', 'transferred', 'transformative', 'travel', 'travelbank', 'traveled', 'traveler', 'travelers', 'travelhelp', 'traveling', 'treat', 'treatment', 'trend', 'tried', 'trip', 'tripping', 'trips', 'trk', 'trouble', 'truly', 'trust', 'trusted', 'truth', 'try', 'trying', 'tsa', 'tsk', 'tsvibtvt', 'tuesday', 'turbulence', 'turn', 'tv', 'tweet', 'twitter', 'two', 'typical', 'tzzjhuibch', 'ua', 'ugh', 'uk', 'ukdjjijrow', 'umpteen', 'unable', 'unacceptable', 'unaccompanied', 'unaccounted', 'unavailable', 'uncomfortable', 'understand', 'understanding', 'unexpected', 'unfamiliar', 'unforeseeable', 'unfortunately', 'unfriendlyskies', 'unhappy', 'unhelpful', 'united', 'unitedairlines', 'unitedbreaksguitars', 'unitedflightsever', 'unless', 'unnecessary', 'unpleased', 'unprofessional', 'unrealistic', 'unrivalled', 'unusable', 'unused', 'unxtqp', 'update', 'updated', 'updates', 'upgrade', 'upgrades', 'uphold', 'upload', 'upon', 'upset', 'ur', 'urgent', 'us', 'usa', 'use', 'used', 'useless', 'user', 'username', 'using', 'ut', 'va', 'vabeatsjblue', 'vacation', 'vail', 'valley', 'vancouver', 'various', 'vbctdlf', 'vc', 'vday', 'vegan', 'vegas', 'vein', 'vendor', 'vents', 'version', 'vf', 'vgn', 'vhp', 'via', 'vibe', 'vice', 'video', 'view', 'views', 'vipswagbags', 'virgin', 'virginamerica', 'virginatlantic', 'visa', 'visors', 'vodkatonics', 'voided', 'volt', 'voucher', 'vouchers', 'vpqem', 'vq', 'vrfhjht', 'vw', 'vx', 'vxsafetydance', 'wait', 'waited', 'waiting', 'waive', 'waived', 'waking', 'walked', 'wallet', 'want', 'wanted', 'wantmymoneyback', 'wants', 'wasappreciated', 'washington', 'waste', 'wasted', 'wastedtime', 'watch', 'watching', 'way', 'ways', 'wc', 'weather', 'web', 'website', 'week', 'weekend', 'weeks', 'welcome', 'well', 'went', 'werin', 'wervirgin', 'whenever', 'white', 'whole', 'wifey', 'wifi', 'willing', 'win', 'windchill', 'window', 'winds', 'wine', 'wing', 'wings', 'winning', 'winter', 'wish', 'within', 'without', 'wks', 'wnd', 'wonder', 'wonderful', 'wondering', 'wonked', 'wont', 'word', 'work', 'worked', 'working', 'works', 'world', 'worm', 'worried', 'worry', 'worrying', 'worse', 'worst', 'worstflightever', 'worth', 'would', 'wow', 'writing', 'wrong', 'wtf', 'wtfodds', 'wu', 'wxb', 'xbqqqbrgvf', 'xc', 'xcvqxykg', 'xelbon', 'xi', 'xoxo', 'xuq', 'xweekly', 'xwjol', 'xz', 'yall', 'ybmbgs', 'yea', 'year', 'years', 'yelling', 'yep', 'yes', 'yesterday', 'yet', 'yg', 'yo', 'york', 'youcouldntmakethis', 'ypo', 'yr', 'yrs', 'yul', 'yummy', 'yvr', 'zambia', 'zcc', 'zero', 'zfqmpgxvs', 'zsuztnaijq', 'zv', 'zy']\n"
     ]
    }
   ],
   "source": [
    "#bag of words with 5000 most common words\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features = 5000)\n",
    "\n",
    "#find the right 5000 words\n",
    "vectorizer.fit(train_tweets)\n",
    "\n",
    "#look at which words it found\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#transform review strings into the rows of word counts\n",
    "#also convert it to a numpy array so we can later feed it to a machine learning algorithm\n",
    "\n",
    "train_word_columns = vectorizer.transform(train_tweets).toarray()\n",
    "test_word_columns = vectorizer.transform(test_tweets).toarray()\n",
    "\n",
    "#look at what train_words_columns looks like\n",
    "print(train_word_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "800\n",
      "0.725\n"
     ]
    }
   ],
   "source": [
    "multinomialNaiveBayes = MultinomialNB()\n",
    "multinomialNaiveBayes.fit(train_word_columns, train_target)\n",
    "print(len(test_word_columns))\n",
    "print(len(train_target))\n",
    "predictions = multinomialNaiveBayes.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "supportVectorClassifier = SVC()\n",
    "supportVectorClassifier.fit(train_word_columns, train_target)\n",
    "predictions = supportVectorClassifier.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameters = {'C':[0.01, 0.1, 1, 10, 100],\n",
    "             'kernel':['rbf', 'linear', 'poly'],\n",
    "             'gamma':[0.01, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_parameters = {'alpha':[0.01, 0.1, 1, 10, 100],\n",
    "                 'fit_prior':[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc = GridSearchCV(SVC(), svc_parameters)\n",
    "svc.fit(train_word_columns, train_target)\n",
    "predictions = svc.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target))\n",
    "print(svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "MultinomialNB(alpha=7, class_prior=None, fit_prior=False)\n"
     ]
    }
   ],
   "source": [
    "mnb = GridSearchCV(MultinomialNB(), mnb_parameters)\n",
    "mnb.fit(train_word_columns, train_target)\n",
    "predictions = mnb.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target))\n",
    "print(mnb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_parameters = {'alpha':[5,6,7,6,8,9,10,11,12,13,14,15],\n",
    "                 'fit_prior':[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "MultinomialNB(alpha=7, class_prior=None, fit_prior=False)\n"
     ]
    }
   ],
   "source": [
    "mnb = GridSearchCV(MultinomialNB(), mnb_parameters)\n",
    "mnb.fit(train_word_columns, train_target)\n",
    "predictions = mnb.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target))\n",
    "print(mnb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_parameters = {'alpha':[6,6.25,6.5,6.75,7,7.25,7.5,7.75,8],\n",
    "                 'fit_prior':[True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "MultinomialNB(alpha=6.75, class_prior=None, fit_prior=False)\n"
     ]
    }
   ],
   "source": [
    "mnb = GridSearchCV(MultinomialNB(), mnb_parameters)\n",
    "mnb.fit(train_word_columns, train_target)\n",
    "predictions = mnb.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target))\n",
    "print(mnb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying some PCA\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "#extractor = PCA(n_components=1, whiten=True)\n",
    "#extractor.fit(data['text'])\n",
    "#print('this is the variance/importance of each component')\n",
    "#print(extractor.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_word_columns, train_target)\n",
    "predictions = gnb.predict(test_word_columns)\n",
    "print(accuracy_score(predictions, test_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
